{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dfb6e40-a560-42c0-b79d-accb7273e601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Documents\\New folder\\Integrated Platform\\ServiceDesk_LLM.py:27: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=api_key)\n",
      "C:\\Users\\Dell\\Documents\\New folder\\Integrated Platform\\ServiceDesk_LLM.py:28: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, openai_api_key=api_key)\n",
      "C:\\Users\\Dell\\Documents\\New folder\\Integrated Platform\\ServiceDesk_LLM.py:113: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vector_stores[source] = Chroma(\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from ServiceDesk_LLM import graph_executor  \n",
    "from gen_ai_script_agent_grafana import gen_ai_script_executer  \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb44e57c-abc1-45d1-9e12-b285155bf920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_2948\\2406325332.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "C:\\Users\\Dell\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\components\\chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Gradio App...\n",
      "* Running on local URL:  http://127.0.0.1:7878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7878/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7878/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "#Chatbot with continuous Chat UI.\n",
    "# Initialize memory for chat history\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Initialize script executor\n",
    "script_executer = gen_ai_script_executer()\n",
    "\n",
    "def ask_chatbot(user_query, history):\n",
    "    \"\"\"Handles continuous chat, stores & retrieves memory.\"\"\"\n",
    "    print(\"Chatbot query received:\", user_query, flush=True)\n",
    "    \n",
    "    # Add \"AI is typing...\" placeholder\n",
    "    history.append((user_query, \"**AI is typing...**\"))\n",
    "    yield \"\", history  # Update UI with \"AI is typing...\"\n",
    "\n",
    "    # Use stored context in the new query\n",
    "    chat_history = memory.load_memory_variables({})\n",
    "    \n",
    "    # Append history to user query for context\n",
    "    full_query = f\"Previous Context:\\n{chat_history['history']}\\n\\nNew Query: {user_query}\"\n",
    "    \n",
    "    # Simulate AI thinking delay (UX improvement)\n",
    "    time.sleep(0.05)\n",
    "\n",
    "    # Invoke LangGraph with context\n",
    "    response = graph_executor.invoke({\"query\": full_query})\n",
    "    chatbot_response = response[\"Final_Response\"]\n",
    "    \n",
    "    # Save current query-response pair to memory\n",
    "    memory.save_context({\"query\": user_query}, {\"response\": chatbot_response})\n",
    "    \n",
    "    print(\"Chatbot response:\", chatbot_response, flush=True)\n",
    "    \n",
    "    # Replace \"AI is typing...\" with actual response\n",
    "    history[-1] = (user_query, chatbot_response)\n",
    "    \n",
    "    yield \"\", history  # Update chat with actual response\n",
    "\n",
    "def run_script(user_query):\n",
    "    \"\"\"Executes a script based on user input.\"\"\"\n",
    "    print(\"Script query received:\", user_query, flush=True)\n",
    "    \n",
    "    result = script_executer.execute_script(user_query)\n",
    "    \n",
    "    print(\"Script execution result:\", result, flush=True)\n",
    "    return result\n",
    "\n",
    "def clear_chat():\n",
    "    \"\"\"Clears chat history.\"\"\"\n",
    "    memory.clear()  # Clears stored memory\n",
    "    return []\n",
    "\n",
    "# 🚀 Define Gradio UI with Enter key submission, typing indicator & clear chat button\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## GenAI-Powered Integrated Platform Environment\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        # Chat Bot Tab\n",
    "        with gr.TabItem(\"SmartOps ChatBot\"):\n",
    "            gr.Markdown(\"### SmartOps AI – Ready to Assist!\")\n",
    "            \n",
    "            chatbot_ui = gr.Chatbot(label=\"Chat History\")  # Continuous chat interface\n",
    "            user_input = gr.Textbox(label=\"Type your message...\", placeholder=\"E.g., How many Windows servers are active?\", show_label=False)\n",
    "            clear_button = gr.Button(\"Clear Chat\")\n",
    "\n",
    "            # Enter Key or Submit Button inside Textbox\n",
    "            user_input.submit(fn=ask_chatbot, inputs=[user_input, chatbot_ui], outputs=[user_input, chatbot_ui])\n",
    "\n",
    "            # Clear Chat Button\n",
    "            clear_button.click(fn=clear_chat, inputs=[], outputs=[chatbot_ui])\n",
    "\n",
    "        # Script Execution Tab\n",
    "        with gr.TabItem(\"Agentic Innobot\"):\n",
    "            gr.Markdown(\"### AI-Powered Action Hub\")\n",
    "            \n",
    "            script_input = gr.Textbox(label=\"Agentic Query\", placeholder='E.g., Please get data of XYZ server from Grafana?', show_label=False)\n",
    "            script_output = gr.Textbox(label=\"Agentic Hub Response\", interactive=False)\n",
    "            \n",
    "            # Enter Key Submission for Script Execution\n",
    "            script_input.submit(fn=run_script, inputs=script_input, outputs=script_output)\n",
    "\n",
    "# 🚀 Launch the Gradio App\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Launching Gradio App...\")\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84541935-0c55-4535-be5d-c542d7872b96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_19948\\1300032901.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Gradio App...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Gradio with ConversationBufferMemory in Chat\n",
    "# script_executer = gen_ai_script_executer()\n",
    "\n",
    "# # Initialize memory (stores previous queries & responses)\n",
    "# memory = ConversationBufferMemory()\n",
    "\n",
    "# def ask_chatbot(user_query):\n",
    "#     \"\"\"Handles user queries, retains memory of previous interactions.\"\"\"\n",
    "#     print(\"Chatbot query received:\", user_query, flush=True)\n",
    "    \n",
    "#     # Use stored context in the new query\n",
    "#     chat_history = memory.load_memory_variables({})\n",
    "    \n",
    "#     # Append history to user query for context\n",
    "#     full_query = f\"Previous Context:\\n{chat_history['history']}\\n\\nNew Query: {user_query}\"\n",
    "    \n",
    "#     # Invoke LangGraph with context\n",
    "#     response = graph_executor.invoke({\"query\": full_query})\n",
    "#     chatbot_response = response[\"Final_Response\"]\n",
    "    \n",
    "#     # Save current query-response pair to memory\n",
    "#     memory.save_context({\"query\": user_query}, {\"response\": chatbot_response})\n",
    "    \n",
    "#     print(\"Chatbot response:\", chatbot_response, flush=True)\n",
    "#     return chatbot_response\n",
    "\n",
    "# # def ask_chatbot(user_query):\n",
    "# #     print(\"Chatbot query received:\", user_query)\n",
    "# #     response = graph_executor.invoke({\"query\": user_query})\n",
    "# #     chatbot_response = response[\"Final_Response\"]\n",
    "# #     print(\"Chatbot response:\", chatbot_response)\n",
    "# #     return chatbot_response\n",
    "\n",
    "# def run_script(user_query):\n",
    "#     print(\"Script query received:\", user_query)\n",
    "#     result = script_executer.execute_script(user_query)\n",
    "#     print(\"Script execution result:\", result)\n",
    "#     return result\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     gr.Markdown(\"## AI-Powered Service Desk\")\n",
    "#     with gr.Tabs():\n",
    "#         with gr.TabItem(\"Chat Bot\"):\n",
    "#             gr.Markdown(\"### Chat with the AI Service Desk\")\n",
    "#             chat_input = gr.Textbox(label=\"Ask a question\", placeholder=\"E.g., How many Windows servers are active?\")\n",
    "#             chat_submit = gr.Button(\"Submit\")\n",
    "#             chat_output = gr.Textbox(label=\"Chat Response\", interactive=False)\n",
    "#             chat_submit.click(fn=ask_chatbot, inputs=chat_input, outputs=chat_output)\n",
    "#         with gr.TabItem(\"Script Execution\"):\n",
    "#             gr.Markdown(\"### Execute a Prevalidated Script\")\n",
    "#             script_input = gr.Textbox(label=\"Script Query\", placeholder='E.g., get data of server name \"abhi1234\" from grafana?')\n",
    "#             script_submit = gr.Button(\"Run Script\")\n",
    "#             script_output = gr.Textbox(label=\"Script Output\", interactive=False)\n",
    "#             script_submit.click(fn=run_script, inputs=script_input, outputs=script_output)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Launching Gradio App...\")\n",
    "#     demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd0592fd-ef90-4abe-a393-af2f1b06a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the script execution agent\n",
    "# script_executer = gen_ai_script_executer()\n",
    "\n",
    "# def ask_chatbot(user_query):\n",
    "#     \"\"\"Send query to the chatbot agent and return its response.\"\"\"\n",
    "#     response = graph_executor.invoke({\"query\": user_query})\n",
    "#     return response[\"Final_Response\"]\n",
    "\n",
    "# def run_script(user_query):\n",
    "#     \"\"\"Execute a prevalidated script based on user query.\"\"\"\n",
    "#     return script_executer.execute_script(user_query)\n",
    "\n",
    "# # Build Gradio UI with two tabs\n",
    "# with gr.Blocks() as demo:\n",
    "#     gr.Markdown(\"## GenAI-Powered Integrated Platform Environment\")\n",
    "    \n",
    "#     with gr.Tabs():\n",
    "#         # Tab 1: Chat Bot\n",
    "#         with gr.TabItem(\"Chat Bot\"):\n",
    "#             gr.Markdown(\"### Chat with the Gen AI Powered Help Desk\")\n",
    "#             chat_input = gr.Textbox(label=\"Ask a question\", placeholder=\"E.g., How many Windows servers are active?\")\n",
    "#             chat_submit = gr.Button(\"Submit\")\n",
    "#             chat_output = gr.Textbox(label=\"Chat Response\", interactive=False)\n",
    "#             chat_submit.click(fn=ask_chatbot, inputs=chat_input, outputs=chat_output)\n",
    "        \n",
    "#         # Tab 2: Script Execution\n",
    "#         with gr.TabItem(\"Script Executer\"):\n",
    "#             gr.Markdown(\"### Exceute System Monitoring Scripts via Gen AI Agent\")\n",
    "#             script_input = gr.Textbox(label=\"Script Query\", placeholder=\"E.g., Get system statistics\")\n",
    "#             script_submit = gr.Button(\"Run Script\")\n",
    "#             script_output = gr.Textbox(label=\"Script Output\", interactive=False)\n",
    "#             script_submit.click(fn=run_script, inputs=script_input, outputs=script_output)\n",
    "\n",
    "# # Run the Gradio app\n",
    "# if __name__ == \"__main__\":\n",
    "#     demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c14ef9ec-1384-461e-b446-48cb90ab6b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7861/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# # Function to handle user queries\n",
    "# def ask_rag_system(user_query):\n",
    "#     \"\"\"Send user query to LangGraph and return response.\"\"\"\n",
    "#     response = graph_executor.invoke({\"query\": user_query})\n",
    "#     return response[\"Final_Response\"]\n",
    "\n",
    "# # Gradio UI\n",
    "# with gr.Blocks() as demo:\n",
    "#     gr.Markdown(\"## 🔍 GEN AI-Powered Integrated Platform Chatbot\")\n",
    "#     with gr.Row():\n",
    "#         input_box = gr.Textbox(label=\"Ask a question\", placeholder=\"E.g., How many total Windows servers are there?\")\n",
    "#         submit_btn = gr.Button(\"Submit\")\n",
    "\n",
    "#     output_box = gr.Textbox(label=\"Response\", interactive=False)\n",
    "\n",
    "#     submit_btn.click(fn=ask_rag_system, inputs=input_box, outputs=output_box)\n",
    "\n",
    "# # Run the Gradio app\n",
    "# if __name__ == \"__main__\":\n",
    "#     demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27d81081-0e32-4820-879b-ae17bef66fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Usage: 66.5%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6072d-3af4-4698-bbb0-b0741d78b8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-llms]",
   "language": "python",
   "name": "conda-env-.conda-llms-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
